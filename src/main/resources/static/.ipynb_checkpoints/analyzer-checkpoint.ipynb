{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42db46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "from parser import get_messages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "# get path to the file to be analyzed\n",
    "# filepath = 'sample.txt'\n",
    "\n",
    "with open('./filepath') as f:\n",
    "    filepath = f.read()\n",
    "\n",
    "# get messages in dataframe\n",
    "msgs = get_messages(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64aec26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert utc timestamp to datetime\n",
    "msgs['date'] = pd.to_datetime(msgs['timestamp'],unit='s')\n",
    "msgs = msgs.drop('timestamp', axis=1)\n",
    "msgs = msgs.drop('platform', axis=1)\n",
    "msgs = msgs.drop('language', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c727c38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get number of messages of each user\n",
    "participants = list(msgs['senderName'].unique())\n",
    "msg_counts = {}\n",
    "\n",
    "for participant in participants:\n",
    "    msg_counts[participant] = len(msgs[msgs['senderName'] == participant])\n",
    "\n",
    "print(msg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f93a6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get timeseries of message/activity over time\n",
    "# x-axis: time in months, y-axis: number of messages \n",
    "msgs2 = msgs\n",
    "msgs2.index = msgs2.date\n",
    "msgs_over_time_values = msgs2.date.resample('M').count()\n",
    "msgs_over_time_values.plot(x=\"time\", y=\"count\")\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a573609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a single string of all messages\n",
    "total_log = ' '.join(msgs['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30ecf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate wordcloud of conversation\n",
    "# %pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# generate and display the word cloud\n",
    "wordcloud = WordCloud().generate(total_log)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518831c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out topic modelling using LDA to generate topics\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8958033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, MiniBatchNMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "\n",
    "n_samples = 20000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 15\n",
    "batch_size = 128\n",
    "init = \"nndsvda\"\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data = msgs['text']\n",
    "data_samples = data #[:n_samples]\n",
    "# print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "# print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "# print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "# print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "# print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b02c79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_hour = msgs.groupby([(msgs.date.dt.hour)]).count()\n",
    "hours = list(counts_by_hour['date'].index)\n",
    "\n",
    "def suffix(time):\n",
    "    if time >= 12:\n",
    "        return \"pm\"\n",
    "    else:\n",
    "        return \"am\"\n",
    "def transform_time(time):\n",
    "    if time%12 == 0:\n",
    "        return 12\n",
    "    else:\n",
    "        return time%12\n",
    "hours = [str(transform_time(h)) + suffix(h) for h in hours]\n",
    "counts = list(counts_by_hour['text'])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(hours, counts)\n",
    "plt.ylim(ymin=0)\n",
    "print(\"Message activity by time of day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e0918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7a4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aedc608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most common emojis\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(s):\n",
    "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "import re\n",
    "import collections\n",
    "emoji_counts = {}\n",
    "\n",
    "# Iterate over the messages and count the emojis used by each user\n",
    "for index, row in msgs.iterrows():\n",
    "    sender = row['senderName']\n",
    "    emojis = emoji.distinct_emoji_list(row['text'])\n",
    "\n",
    "    for item in emojis:\n",
    "        if sender not in emoji_counts:\n",
    "            emoji_counts[sender] = collections.defaultdict(int)\n",
    "        emoji_counts[sender][item] += 1\n",
    "\n",
    "# Print the most common emoji for each user\n",
    "import operator\n",
    "for sender in emoji_counts.keys():\n",
    "    vals = emoji_counts[sender]\n",
    "    res = {k: v for k, v in sorted(vals.items(), key=lambda item: item[1])}\n",
    "    most_common = list(res.keys())\n",
    "    most_common.reverse()\n",
    "    print(f\"Most used emojis by {sender}: {most_common[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96e55146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the messages and run sentiment analysis on each line\n",
    "import requests\n",
    "\n",
    "sentiments = []\n",
    "url = \"http://127.0.0.1:3000/sentiment\"\n",
    "for index, row in msgs.iterrows():\n",
    "    sender = row['senderName']\n",
    "\n",
    "    # make API call to get sentiment\n",
    "    resp = requests.post(url, data={'input': row['text']})\n",
    "    sentiment = resp.json()\n",
    "    sentiments.append({'date': row['date'], 'type': sentiment[0]['label'], 'score': sentiment[0]['score']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e44865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the sentiments records\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    if sentiments[idx]['type'] == 'NEGATIVE':\n",
    "        sentiments[idx]['score'] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6312d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sentiment over time\n",
    "xaxis = [idx for idx in range(len(sentiments))]\n",
    "yaxis = [x['score'] for x in sentiments]\n",
    "plt.bar(xaxis, yaxis, color = list(map(lambda x: 'g' if x > 0 else 'r', yaxis))) # blue/red for positive/negative bars\n",
    "plt.title(\"sentiment over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096ebbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
